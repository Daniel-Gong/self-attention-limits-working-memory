This is the repo for NerIPS 2024 Workshop Paper "[Self-attention limits working memory capacity of transformer-based models](https://arxiv.org/abs/2409.10715)".
